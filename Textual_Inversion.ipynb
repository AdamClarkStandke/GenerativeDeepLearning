{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamClarkStandke/GenerativeDeepLearning/blob/main/Textual_Inversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgFp-3sCAUxa"
      },
      "source": [
        "# Textual Inversion Background\n",
        "\n",
        "I will be implementing keras' tutorial of [Teach StableDiffusion new concepts via Textual Inversion](https://keras.io/examples/generative/fine_tune_via_textual_inversion/) in which new visual concepts are learned with KerasCV's StableDiffusion implementation. As the authors of [Teach StableDiffusion new concepts via Textual Inversion](https://keras.io/examples/generative/fine_tune_via_textual_inversion/) state:\n",
        "\n",
        "> Textual Inversion is the process of teaching an image generator a specific visual concept through the use of fine-tuning...Conceptually, textual inversion works by learning a token embedding for a new text token, keeping the remaining components of StableDiffusion frozen.\n",
        "\n",
        "This inversion method came about from the paper [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/pdf/2208.01618.pdf). As the authors of the paper state:\n",
        "\n",
        "\n",
        "> Text-to-image models offer unprecedented freedom to guide creation through natural language. **Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts**, modify their appearance, or compose them in new roles and novel scenes...[h]ere **we present a simple\n",
        "approach that allows such creative freedom. Using only 3-5 images** of a user-provided concept, like an object or a style, **we learn to represent it through new “words” in the embedding space** of a frozen text-to-image model.\n",
        "\n",
        "The objective of textual inversion is to learn **a single embedding** for a  pseudo-word so that the pseudo-word can be treated like any other word when conditioning the Reverse Diffusion Process. As the authors of the paper [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/pdf/2208.01618.pdf) state:\n",
        "\n",
        "\n",
        "> We represent a new embedding vector with a new pseudo-word...which we denote by $S_*$...This pseudo-word is then treated like any other word, and can be used to compose novel textual queries for the generative models.\n",
        "\n",
        "> To find these pseudo-words, we frame the task as one of inversion. We are given a fixed, pre-trained text-toimage model and a small (3-5) image set depicting the concept. We aim to find a single word embedding, such that sentences of the form “A photo of $S_*$” will lead to the reconstruction of images from our small set.\n",
        "\n",
        "Mathematically speaking, the task is one of finding an embedding vector $v_*$ that minimizes the following [objective](https://arxiv.org/pdf/2208.01618.pdf):\n",
        "\n",
        "\\begin{align} v_* = arg min_v \\mathbb{E}_{E(x),y,\\epsilon\\sim N(0,1), t}[\\parallel \\epsilon-\\epsilon_{\\theta}(z_t, t, τ_{θ}(y))\\parallel^2_2] \\end{align}\n",
        "\n",
        "where both $\\epsilon_{\\theta}$ and $τ_{θ}$ are fixed.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBmJnQkF__WR",
        "outputId": "5dcd7681-81b2-4286-87d8-126429a473f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n",
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow -q\n",
        "!pip install keras-cv==0.6.0 -q\n",
        "!pip install keras-core -q\n",
        "\n",
        "import math\n",
        "import string\n",
        "import random\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_cv import layers as cv_layers\n",
        "from keras_cv.models.stable_diffusion import NoiseScheduler\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stable_diffusion = keras_cv.models.StableDiffusion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-opTMtMtDsZ_"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.savefig(''.join(random.choices(string.ascii_uppercase + string.digits, k=10))+'.png', bbox_inches='tight')\n",
        "        plt.imshow(images[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assembling a text-image pair dataset\n",
        "\n",
        "As the authors of the keras' tutorial of [Teach StableDiffusion new concepts via Textual Inversion](https://keras.io/examples/generative/fine_tune_via_textual_inversion/) detail:\n",
        "\n",
        "\n",
        "> In order to train the embedding of our new token, we first must assemble a dataset consisting of text-image pairs. Each sample from the dataset must contain an image of the concept we are teaching StableDiffusion, as well as a caption accurately representing the content of the image.\n",
        "\n",
        "Interestingly the authors detail the importance of **accurate prompts** (or as the authors of the paper [An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/pdf/2208.01618.pdf) call the *template prompts*):\n",
        "\n",
        "> During our first attempt at writing this guide we included images of groups of these cat dolls in our dataset but continued to use the generic prompts listed above. Our results were anecdotally poor...[i]n order to remedy this, we began experimenting with splitting our images into images of singular cat dolls and groups of cat dolls. Following this split, we came up with new prompts for the group shots.**Training on text-image pairs that accurately represent the content boosted the quality of our results substantially.** This speaks to the importance of prompt accuracy.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m0LfJ1OFBLJL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CQflp-0D0zn"
      },
      "outputs": [],
      "source": [
        "def assemble_image_dataset(urls):\n",
        "    # Fetch all remote files\n",
        "    files = [tf.keras.utils.get_file(origin=url) for url in urls]\n",
        "\n",
        "    # Resize images\n",
        "    resize = keras.layers.Resizing(height=512, width=512, crop_to_aspect_ratio=True)\n",
        "    images = [keras.utils.load_img(img) for img in files]\n",
        "    images = [keras.utils.img_to_array(img) for img in images]\n",
        "    images = np.array([resize(img) for img in images])\n",
        "\n",
        "    # The StableDiffusion image encoder requires images to be normalized to the\n",
        "    # [-1, 1] pixel value range\n",
        "    images = images / 127.5 - 1\n",
        "\n",
        "    # Create the tf.data.Dataset\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "\n",
        "    # Shuffle and introduce random noise\n",
        "    image_dataset = image_dataset.shuffle(50, reshuffle_each_iteration=True)\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomCropAndResize(\n",
        "            target_size=(512, 512),\n",
        "            crop_area_factor=(0.8, 1.0),\n",
        "            aspect_ratio_factor=(1.0, 1.0),\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomFlip(mode=\"horizontal\"),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    return image_dataset\n",
        "\n",
        "MAX_PROMPT_LENGTH = 77\n",
        "placeholder_token = \"<my-sexy-face-token>\"\n",
        "\n",
        "\n",
        "def pad_embedding(embedding):\n",
        "    return embedding + (\n",
        "        [stable_diffusion.tokenizer.end_of_text] * (MAX_PROMPT_LENGTH - len(embedding))\n",
        "    )\n",
        "\n",
        "\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "\n",
        "def assemble_text_dataset(prompts):\n",
        "    prompts = [prompt.format(placeholder_token) for prompt in prompts]\n",
        "    embeddings = [stable_diffusion.tokenizer.encode(prompt) for prompt in prompts]\n",
        "    embeddings = [np.array(pad_embedding(embedding)) for embedding in embeddings]\n",
        "    text_dataset = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    text_dataset = text_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "    return text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ziv9SfUF6z8"
      },
      "outputs": [],
      "source": [
        "def assemble_dataset(urls, prompts):\n",
        "    image_dataset = assemble_image_dataset(urls)\n",
        "    text_dataset = assemble_text_dataset(prompts)\n",
        "    # the image dataset is quite short, so we repeat it to match the length of the\n",
        "    # text prompt dataset\n",
        "    image_dataset = image_dataset.repeat()\n",
        "    # we use the text prompt dataset to determine the length of the dataset.  Due to\n",
        "    # the fact that there are relatively few prompts we repeat the dataset 5 times.\n",
        "    # we have found that this anecdotally improves results.\n",
        "    text_dataset = text_dataset.repeat(5)\n",
        "    return tf.data.Dataset.zip((image_dataset, text_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1xTgGQZGGXm"
      },
      "outputs": [],
      "source": [
        "single_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/xR6Jl5u.jpg\",\n",
        "        \"https://i.imgur.com/Jm3A3Wg.jpg\",\n",
        "        \"https://i.imgur.com/onznz55.jpg\",\n",
        "        \"https://i.imgur.com/4Rf4MJB.jpg\",\n",
        "        \"https://i.imgur.com/CSnJH7f.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ],\n",
        ")\n",
        "# train_ds = train_ds.batch(1).shuffle(\n",
        "#     train_ds.cardinality(), reshuffle_each_iteration=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "group_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/bDCCWn6.png\",\n",
        "        \"https://i.imgur.com/TUgHrJm.png\",\n",
        "        \"https://i.imgur.com/283nK4Z.png\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a group of {}\",\n",
        "        \"a rendering of a group of {}\",\n",
        "        \"a cropped photo of the group of {}\",\n",
        "        \"the photo of a group of {}\",\n",
        "        \"a photo of a clean group of {}\",\n",
        "        \"a photo of my group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a close-up photo of a group of {}\",\n",
        "        \"a bright photo of the group of {}\",\n",
        "        \"a cropped photo of a group of {}\",\n",
        "        \"a photo of the group of {}\",\n",
        "        \"a good photo of the group of {}\",\n",
        "        \"a photo of one group of {}\",\n",
        "        \"a close-up photo of the group of {}\",\n",
        "        \"a rendition of the group of {}\",\n",
        "        \"a photo of the clean group of {}\",\n",
        "        \"a rendition of a group of {}\",\n",
        "        \"a photo of a nice group of {}\",\n",
        "        \"a good photo of a group of {}\",\n",
        "        \"a photo of the nice group of {}\",\n",
        "        \"a photo of the small group of {}\",\n",
        "        \"a photo of the weird group of {}\",\n",
        "        \"a photo of the large group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a photo of a small group of {}\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "b10KFLOY2N9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = single_ds.concatenate(group_ds)\n",
        "train_ds = train_ds.batch(1).shuffle(\n",
        "    train_ds.cardinality(), reshuffle_each_iteration=True\n",
        ")"
      ],
      "metadata": {
        "id": "ILgWn3pE36BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a new token to the text encoder\n",
        "\n",
        "**This part is very key** **since this basically expands the pre-trained** [CLIP](https://paperswithcode.com/method/clip) **token embedding space** so that our token can be added to this space. **Even though the embedding space has already been pre-trained we add our token vector to this space and only train on this vector while all the other token vectors in the embedding space are frozen**. This is also necessary when saving the embedding weights of embedding layer and then reloding them back into Stable Diffusion's textual encoder."
      ],
      "metadata": {
        "id": "FPup34YLGO0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbRtKDClHCJy"
      },
      "outputs": [],
      "source": [
        "tokenized_initializer = stable_diffusion.tokenizer.encode(\"face\")[1]\n",
        "\n",
        "# creating our trainable token vector (i.e. v*)\n",
        "new_weights = stable_diffusion.text_encoder.layers[2].token_embedding(\n",
        "    tf.constant(tokenized_initializer)\n",
        ")\n",
        "\n",
        "# Get len of .vocab instead of tokenizer\n",
        "new_vocab_size = len(stable_diffusion.tokenizer.vocab)\n",
        "\n",
        "# The embedding layer is the 2nd layer in the text encoder\n",
        "old_token_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].token_embedding.get_weights()\n",
        "old_position_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].position_embedding.get_weights()\n",
        "\n",
        "old_token_weights = old_token_weights[0]\n",
        "new_weights = np.expand_dims(new_weights, axis=0)\n",
        "\n",
        "# combining our traininable vector with old embedding weights\n",
        "new_weights = np.concatenate([old_token_weights, new_weights], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I01_-t_eJTDa"
      },
      "outputs": [],
      "source": [
        "# Have to set download_weights False so we can init (otherwise tries to load weights)\n",
        "new_encoder = keras_cv.models.stable_diffusion.TextEncoder(\n",
        "    keras_cv.models.stable_diffusion.stable_diffusion.MAX_PROMPT_LENGTH,\n",
        "    vocab_size=new_vocab_size,\n",
        "    download_weights=False,\n",
        ")\n",
        "for index, layer in enumerate(stable_diffusion.text_encoder.layers):\n",
        "    # Layer 2 is the embedding layer, so we omit it from our weight-copying\n",
        "    if index == 2:\n",
        "        continue\n",
        "    new_encoder.layers[index].set_weights(layer.get_weights())\n",
        "\n",
        "# the new embedding layer with our trainable token vector included\n",
        "new_encoder.layers[2].token_embedding.set_weights([new_weights])\n",
        "new_encoder.layers[2].position_embedding.set_weights(old_position_weights)\n",
        "\n",
        "stable_diffusion._text_encoder = new_encoder\n",
        "stable_diffusion._text_encoder.compile(jit_compile=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wfip4PVSLi1"
      },
      "outputs": [],
      "source": [
        "stable_diffusion.diffusion_model.trainable = False\n",
        "stable_diffusion.decoder.trainable = False\n",
        "stable_diffusion.text_encoder.trainable = True\n",
        "\n",
        "stable_diffusion.text_encoder.layers[2].trainable = True\n",
        "\n",
        "\n",
        "def traverse_layers(layer):\n",
        "    if hasattr(layer, \"layers\"):\n",
        "        for layer in layer.layers:\n",
        "            yield layer\n",
        "    if hasattr(layer, \"token_embedding\"):\n",
        "        yield layer.token_embedding\n",
        "    if hasattr(layer, \"position_embedding\"):\n",
        "        yield layer.position_embedding\n",
        "\n",
        "\n",
        "for layer in traverse_layers(stable_diffusion.text_encoder):\n",
        "    if isinstance(layer, keras.layers.Embedding) or \"clip_embedding\" in layer.name:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "\n",
        "new_encoder.layers[2].position_embedding.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the new embedding"
      ],
      "metadata": {
        "id": "jj7z2NmXWjBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMpJjgF7TuyU"
      },
      "outputs": [],
      "source": [
        "# Remove the top layer from the encoder, which cuts off the variance and only returns\n",
        "# the mean\n",
        "training_image_encoder = keras.Model(\n",
        "    stable_diffusion.image_encoder.input,\n",
        "    stable_diffusion.image_encoder.layers[-2].output,\n",
        ")\n",
        "\n",
        "\n",
        "def sample_from_encoder_outputs(outputs):\n",
        "    mean, logvar = tf.split(outputs, 2, axis=-1)\n",
        "    logvar = tf.clip_by_value(logvar, -30.0, 20.0)\n",
        "    std = tf.exp(0.5 * logvar)\n",
        "    sample = tf.random.normal(tf.shape(mean))\n",
        "    return mean + std * sample\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timestep, dim=320, max_period=10000):\n",
        "    half = dim // 2\n",
        "    freqs = tf.math.exp(\n",
        "        -math.log(max_period) * tf.range(0, half, dtype=tf.float32) / half\n",
        "    )\n",
        "    args = tf.convert_to_tensor([timestep], dtype=tf.float32) * freqs\n",
        "    embedding = tf.concat([tf.math.cos(args), tf.math.sin(args)], 0)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def get_position_ids():\n",
        "    return tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96xWzdeLUDkf"
      },
      "outputs": [],
      "source": [
        "class StableDiffusionFineTuner(keras.Model):\n",
        "    def __init__(self, stable_diffusion, noise_scheduler, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stable_diffusion = stable_diffusion\n",
        "        self.noise_scheduler = noise_scheduler\n",
        "\n",
        "    def train_step(self, data):\n",
        "        images, embeddings = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Sample from the predicted distribution for the training image\n",
        "            latents = sample_from_encoder_outputs(training_image_encoder(images))\n",
        "            # The latents must be downsampled to match the scale of the latents used\n",
        "            # in the training of StableDiffusion.  This number is truly just a \"magic\"\n",
        "            # constant that they chose when training the model.\n",
        "            latents = latents * 0.18215\n",
        "\n",
        "            # Produce random noise in the same shape as the latent sample\n",
        "            noise = tf.random.normal(tf.shape(latents))\n",
        "            batch_dim = tf.shape(latents)[0]\n",
        "\n",
        "            # Pick a random timestep for each sample in the batch\n",
        "            timesteps = tf.random.uniform(\n",
        "                (batch_dim,),\n",
        "                minval=0,\n",
        "                maxval=noise_scheduler.train_timesteps,\n",
        "                dtype=tf.int64,\n",
        "            )\n",
        "\n",
        "            # Add noise to the latents based on the timestep for each sample\n",
        "            noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Encode the text in the training samples to use as hidden state in the\n",
        "            # diffusion model\n",
        "            encoder_hidden_state = self.stable_diffusion.text_encoder(\n",
        "                [embeddings, get_position_ids()]\n",
        "            )\n",
        "\n",
        "            # Compute timestep embeddings for the randomly-selected timesteps for each\n",
        "            # sample in the batch\n",
        "            timestep_embeddings = tf.map_fn(\n",
        "                fn=get_timestep_embedding,\n",
        "                elems=timesteps,\n",
        "                fn_output_signature=tf.float32,\n",
        "            )\n",
        "\n",
        "            # Call the diffusion model\n",
        "            noise_pred = self.stable_diffusion.diffusion_model(\n",
        "                [noisy_latents, timestep_embeddings, encoder_hidden_state]\n",
        "            )\n",
        "\n",
        "            # Compute the mean-squared error loss and reduce it.\n",
        "            loss = self.compiled_loss(noise_pred, noise)\n",
        "            loss = tf.reduce_mean(loss, axis=2)\n",
        "            loss = tf.reduce_mean(loss, axis=1)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Load the trainable weights and compute the gradients for them\n",
        "        trainable_weights = self.stable_diffusion.text_encoder.trainable_weights\n",
        "        grads = tape.gradient(loss, trainable_weights)\n",
        "\n",
        "        # Gradients are stored in indexed slices, so we have to find the index\n",
        "        # of the slice(s) which contain the placeholder token.\n",
        "        index_of_placeholder_token = tf.reshape(tf.where(grads[0].indices == 49408), ())\n",
        "        condition = grads[0].indices == 49408\n",
        "        condition = tf.expand_dims(condition, axis=-1)\n",
        "\n",
        "        # Override the gradients, zeroing out the gradients for all slices that\n",
        "        # aren't for the placeholder token, effectively freezing the weights for\n",
        "        # all other tokens.\n",
        "        grads[0] = tf.IndexedSlices(\n",
        "            values=tf.where(condition, grads[0].values, 0),\n",
        "            indices=grads[0].indices,\n",
        "            dense_shape=grads[0].dense_shape,\n",
        "        )\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
        "        return {\"loss\": loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QaCUZbyVgH6",
        "outputId": "993c541b-60e5-45a3-fa0b-db9910ea1d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 257s 646ms/step - loss: 0.1459\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 167s 655ms/step - loss: 0.1214\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 163s 644ms/step - loss: 0.1425\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 161s 633ms/step - loss: 0.1335\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 142s 555ms/step - loss: 0.1480\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 130s 510ms/step - loss: 0.1408\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 131s 510ms/step - loss: 0.1444\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 130s 511ms/step - loss: 0.1497\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1433\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 130s 509ms/step - loss: 0.1359\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1445\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1459\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1561\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 130s 509ms/step - loss: 0.1517\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 130s 511ms/step - loss: 0.1578\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1346\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1428\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1352\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1419\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1313\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 130s 506ms/step - loss: 0.1530\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1506\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1461\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1372\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 131s 511ms/step - loss: 0.1394\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1416\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1273\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1330\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1430\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1351\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1363\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1494\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1470\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1309\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1444\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1274\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1547\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1636\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1595\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1386\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1486\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1339\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1448\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 129s 505ms/step - loss: 0.1480\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 129s 506ms/step - loss: 0.1530\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 130s 508ms/step - loss: 0.1510\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 130s 507ms/step - loss: 0.1474\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 129s 507ms/step - loss: 0.1292\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 130s 506ms/step - loss: 0.1266\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 129s 505ms/step - loss: 0.1283\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e28bc82cca0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "noise_scheduler = NoiseScheduler(\n",
        "    beta_start=0.00085,\n",
        "    beta_end=0.012,\n",
        "    beta_schedule=\"scaled_linear\",\n",
        "    train_timesteps=1000,\n",
        ")\n",
        "trainer = StableDiffusionFineTuner(stable_diffusion, noise_scheduler, name=\"trainer\")\n",
        "EPOCHS = 50\n",
        "learning_rate = keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-4, decay_steps=train_ds.cardinality() * EPOCHS\n",
        ")\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    weight_decay=0.004, learning_rate=learning_rate, epsilon=1e-8, global_clipnorm=10\n",
        ")\n",
        "\n",
        "trainer.compile(\n",
        "    optimizer=optimizer,\n",
        "    # We are performing reduction manually in our train step, so none is required here.\n",
        "    loss=keras.losses.MeanSquaredError(reduction=\"none\"),\n",
        ")\n",
        "\n",
        "class GenerateImages(keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self, stable_diffusion, prompt, steps=50, frequency=10, seed=None, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stable_diffusion = stable_diffusion\n",
        "        self.prompt = prompt\n",
        "        self.seed = seed\n",
        "        self.frequency = frequency\n",
        "        self.steps = steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % self.frequency == 0:\n",
        "            images = self.stable_diffusion.text_to_image(\n",
        "                self.prompt, batch_size=3, num_steps=self.steps, seed=self.seed\n",
        "            )\n",
        "            plot_images(\n",
        "                images,\n",
        "            )\n",
        "\n",
        "\n",
        "cbs = [\n",
        "    GenerateImages(\n",
        "        stable_diffusion, prompt=f\"an oil painting of {placeholder_token}\", seed=1337\n",
        "    ),\n",
        "    GenerateImages(\n",
        "        stable_diffusion, prompt=f\"gandalf the gray as a {placeholder_token}\", seed=1337\n",
        "    ),\n",
        "    GenerateImages(\n",
        "        stable_diffusion,\n",
        "        prompt=f\"two {placeholder_token} getting married, photorealistic, high quality\",\n",
        "        seed=1337,\n",
        "    ),\n",
        "]\n",
        "\n",
        "trainer.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    # callbacks=cbs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Weights of the Embedding Layer\n",
        "\n",
        "In order to use the embedding layer with our included trained token, I save it as a numpy array for later reuse!!!"
      ],
      "metadata": {
        "id": "UpzP3761ZGYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_face_weights=stable_diffusion.text_encoder.layers[2].get_weights()\n",
        "np.save(\"/content/drive/MyDrive/pretrained.npy\", my_face_weights)"
      ],
      "metadata": {
        "id": "_VZyN5JLvJWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a40957-5ab1-4949-9632-07e4b766bd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = np.asanyarray(arr)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1WdJpDs8IJ25mNnjHFQ6pPtbCcIHJEXBi",
      "authorship_tag": "ABX9TyO+HyFgKlKDPTCei49HV4PG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}