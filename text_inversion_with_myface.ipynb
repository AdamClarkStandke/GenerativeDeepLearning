{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamClarkStandke/GenerativeDeepLearning/blob/main/text_inversion_with_myface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgFp-3sCAUxa"
      },
      "source": [
        "# Textual Inversion With My Face:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9CQflp-0D0zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6452ad05-06fc-4b5a-ca60-a12b0f387bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.3/756.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing TensorFlow backend\n",
            "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n",
            "Downloading data from https://github.com/openai/CLIP/blob/main/clip/bpe_simple_vocab_16e6.txt.gz?raw=true\n",
            "1356917/1356917 [==============================] - 1s 0us/step\n",
            "Downloading data from https://i.imgur.com/xR6Jl5u.jpg\n",
            "400102/400102 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/Jm3A3Wg.jpg\n",
            "398823/398823 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/onznz55.jpg\n",
            "302731/302731 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/4Rf4MJB.jpg\n",
            "231712/231712 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/CSnJH7f.jpg\n",
            "215775/215775 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/bDCCWn6.png\n",
            "1080919/1080919 [==============================] - 1s 1us/step\n",
            "Downloading data from https://i.imgur.com/TUgHrJm.png\n",
            "2666051/2666051 [==============================] - 0s 0us/step\n",
            "Downloading data from https://i.imgur.com/283nK4Z.png\n",
            "2088041/2088041 [==============================] - 0s 0us/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_encoder.h5\n",
            "492466864/492466864 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorflow -q\n",
        "!pip install keras-cv==0.6.0 -q\n",
        "!pip install keras-core -q\n",
        "\n",
        "import math\n",
        "import string\n",
        "import random\n",
        "import keras_cv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_cv import layers as cv_layers\n",
        "from keras_cv.models.stable_diffusion import NoiseScheduler\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_core import ops\n",
        "from PIL import Image\n",
        "\n",
        "stable_diffusion = keras_cv.models.StableDiffusion()\n",
        "\n",
        "def export_as_gif(filename, images, frames_per_second=30, rubber_band=False):\n",
        "    if rubber_band:\n",
        "        images += images[2:-1][::-1]\n",
        "    images[0].save(\n",
        "        filename,\n",
        "        save_all=True,\n",
        "        append_images=images[1:],\n",
        "        duration=1000 // frames_per_second,\n",
        "        loop=0,\n",
        "    )\n",
        "\n",
        "def downsample(image):\n",
        "  half = 0.5\n",
        "  size = [int(half * s) for s in image.size]\n",
        "  return image.resize(size)\n",
        "\n",
        "def plot_images(images):\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    for i in range(len(images)):\n",
        "        ax = plt.subplot(1, len(images), i + 1)\n",
        "        plt.axis(\"off\")\n",
        "        plt.savefig(''.join(random.choices(string.ascii_uppercase + string.digits, k=10))+'.png', bbox_inches='tight')\n",
        "        plt.imshow(images[i])\n",
        "\n",
        "def assemble_image_dataset(urls):\n",
        "    # Fetch all remote files\n",
        "    files = [tf.keras.utils.get_file(origin=url) for url in urls]\n",
        "\n",
        "    # Resize images\n",
        "    resize = keras.layers.Resizing(height=512, width=512, crop_to_aspect_ratio=True)\n",
        "    images = [keras.utils.load_img(img) for img in files]\n",
        "    images = [keras.utils.img_to_array(img) for img in images]\n",
        "    images = np.array([resize(img) for img in images])\n",
        "\n",
        "    # The StableDiffusion image encoder requires images to be normalized to the\n",
        "    # [-1, 1] pixel value range\n",
        "    images = images / 127.5 - 1\n",
        "\n",
        "    # Create the tf.data.Dataset\n",
        "    image_dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "\n",
        "    # Shuffle and introduce random noise\n",
        "    image_dataset = image_dataset.shuffle(50, reshuffle_each_iteration=True)\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomCropAndResize(\n",
        "            target_size=(512, 512),\n",
        "            crop_area_factor=(0.8, 1.0),\n",
        "            aspect_ratio_factor=(1.0, 1.0),\n",
        "        ),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    image_dataset = image_dataset.map(\n",
        "        cv_layers.RandomFlip(mode=\"horizontal\"),\n",
        "        num_parallel_calls=tf.data.AUTOTUNE,\n",
        "    )\n",
        "    return image_dataset\n",
        "\n",
        "MAX_PROMPT_LENGTH = 77\n",
        "placeholder_token = \"<my-sexy-face-token>\"\n",
        "\n",
        "\n",
        "def pad_embedding(embedding):\n",
        "    return embedding + (\n",
        "        [stable_diffusion.tokenizer.end_of_text] * (MAX_PROMPT_LENGTH - len(embedding))\n",
        "    )\n",
        "\n",
        "\n",
        "stable_diffusion.tokenizer.add_tokens(placeholder_token)\n",
        "\n",
        "\n",
        "def assemble_text_dataset(prompts):\n",
        "    prompts = [prompt.format(placeholder_token) for prompt in prompts]\n",
        "    embeddings = [stable_diffusion.tokenizer.encode(prompt) for prompt in prompts]\n",
        "    embeddings = [np.array(pad_embedding(embedding)) for embedding in embeddings]\n",
        "    text_dataset = tf.data.Dataset.from_tensor_slices(embeddings)\n",
        "    text_dataset = text_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
        "    return text_dataset\n",
        "\n",
        "def assemble_dataset(urls, prompts):\n",
        "    image_dataset = assemble_image_dataset(urls)\n",
        "    text_dataset = assemble_text_dataset(prompts)\n",
        "    # the image dataset is quite short, so we repeat it to match the length of the\n",
        "    # text prompt dataset\n",
        "    image_dataset = image_dataset.repeat()\n",
        "    # we use the text prompt dataset to determine the length of the dataset.  Due to\n",
        "    # the fact that there are relatively few prompts we repeat the dataset 5 times.\n",
        "    # we have found that this anecdotally improves results.\n",
        "    text_dataset = text_dataset.repeat(5)\n",
        "    return tf.data.Dataset.zip((image_dataset, text_dataset))\n",
        "\n",
        "single_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/xR6Jl5u.jpg\",\n",
        "        \"https://i.imgur.com/Jm3A3Wg.jpg\",\n",
        "        \"https://i.imgur.com/onznz55.jpg\",\n",
        "        \"https://i.imgur.com/4Rf4MJB.jpg\",\n",
        "        \"https://i.imgur.com/CSnJH7f.jpg\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a {}\",\n",
        "        \"a rendering of a {}\",\n",
        "        \"a cropped photo of the {}\",\n",
        "        \"the photo of a {}\",\n",
        "        \"a photo of a clean {}\",\n",
        "        \"a photo of my {}\",\n",
        "        \"a photo of the cool {}\",\n",
        "        \"a close-up photo of a {}\",\n",
        "        \"a bright photo of the {}\",\n",
        "        \"a cropped photo of a {}\",\n",
        "        \"a photo of the {}\",\n",
        "        \"a good photo of the {}\",\n",
        "        \"a photo of one {}\",\n",
        "        \"a close-up photo of the {}\",\n",
        "        \"a rendition of the {}\",\n",
        "        \"a photo of the clean {}\",\n",
        "        \"a rendition of a {}\",\n",
        "        \"a photo of a nice {}\",\n",
        "        \"a good photo of a {}\",\n",
        "        \"a photo of the nice {}\",\n",
        "        \"a photo of the small {}\",\n",
        "        \"a photo of the weird {}\",\n",
        "        \"a photo of the large {}\",\n",
        "        \"a photo of a cool {}\",\n",
        "        \"a photo of a small {}\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "group_ds = assemble_dataset(\n",
        "    urls=[\n",
        "        \"https://i.imgur.com/bDCCWn6.png\",\n",
        "        \"https://i.imgur.com/TUgHrJm.png\",\n",
        "        \"https://i.imgur.com/283nK4Z.png\",\n",
        "    ],\n",
        "    prompts=[\n",
        "        \"a photo of a group of {}\",\n",
        "        \"a rendering of a group of {}\",\n",
        "        \"a cropped photo of the group of {}\",\n",
        "        \"the photo of a group of {}\",\n",
        "        \"a photo of a clean group of {}\",\n",
        "        \"a photo of my group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a close-up photo of a group of {}\",\n",
        "        \"a bright photo of the group of {}\",\n",
        "        \"a cropped photo of a group of {}\",\n",
        "        \"a photo of the group of {}\",\n",
        "        \"a good photo of the group of {}\",\n",
        "        \"a photo of one group of {}\",\n",
        "        \"a close-up photo of the group of {}\",\n",
        "        \"a rendition of the group of {}\",\n",
        "        \"a photo of the clean group of {}\",\n",
        "        \"a rendition of a group of {}\",\n",
        "        \"a photo of a nice group of {}\",\n",
        "        \"a good photo of a group of {}\",\n",
        "        \"a photo of the nice group of {}\",\n",
        "        \"a photo of the small group of {}\",\n",
        "        \"a photo of the weird group of {}\",\n",
        "        \"a photo of the large group of {}\",\n",
        "        \"a photo of a cool group of {}\",\n",
        "        \"a photo of a small group of {}\",\n",
        "    ],\n",
        ")\n",
        "train_ds = single_ds.concatenate(group_ds)\n",
        "train_ds = train_ds.batch(1).shuffle(\n",
        "    train_ds.cardinality(), reshuffle_each_iteration=True\n",
        ")\n",
        "\n",
        "tokenized_initializer = stable_diffusion.tokenizer.encode(\"face\")[1]\n",
        "new_weights = stable_diffusion.text_encoder.layers[2].token_embedding(\n",
        "    tf.constant(tokenized_initializer)\n",
        ")\n",
        "\n",
        "# Get len of .vocab instead of tokenizer\n",
        "new_vocab_size = len(stable_diffusion.tokenizer.vocab)\n",
        "\n",
        "# The embedding layer is the 2nd layer in the text encoder\n",
        "old_token_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].token_embedding.get_weights()\n",
        "old_position_weights = stable_diffusion.text_encoder.layers[\n",
        "    2\n",
        "].position_embedding.get_weights()\n",
        "\n",
        "old_token_weights = old_token_weights[0]\n",
        "new_weights = np.expand_dims(new_weights, axis=0)\n",
        "new_weights = np.concatenate([old_token_weights, new_weights], axis=0)\n",
        "\n",
        "# Have to set download_weights False so we can init (otherwise tries to load weights)\n",
        "new_encoder = keras_cv.models.stable_diffusion.TextEncoder(\n",
        "    keras_cv.models.stable_diffusion.stable_diffusion.MAX_PROMPT_LENGTH,\n",
        "    vocab_size=new_vocab_size,\n",
        "    download_weights=False,\n",
        ")\n",
        "for index, layer in enumerate(stable_diffusion.text_encoder.layers):\n",
        "    # Layer 2 is the embedding layer, so we omit it from our weight-copying\n",
        "    if index == 2:\n",
        "        continue\n",
        "    new_encoder.layers[index].set_weights(layer.get_weights())\n",
        "\n",
        "\n",
        "new_encoder.layers[2].token_embedding.set_weights([new_weights])\n",
        "new_encoder.layers[2].position_embedding.set_weights(old_position_weights)\n",
        "\n",
        "stable_diffusion._text_encoder = new_encoder\n",
        "stable_diffusion._text_encoder.compile(jit_compile=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Application"
      ],
      "metadata": {
        "id": "N1rHyNeZqrM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained=np.load(\"/content/drive/MyDrive/pretrained.npy\", allow_pickle=True)\n",
        "stable_diffusion.text_encoder.layers[2].set_weights(pretrained)\n",
        "prompt =  f\"man in fancy suit with {placeholder_token} walking in New York high quality, highly detailed, elegant, sharp focus, adventure\"\n",
        "encoding=stable_diffusion.encode_text(prompt)\n",
        "noise = tf.random.normal((512 // 8, 512 // 8, 4))\n",
        "\n",
        "walk_steps = 150\n",
        "batch_size = 3\n",
        "batches = walk_steps // batch_size\n",
        "\n",
        "walk_noise_x = tf.random.normal(noise.shape, dtype=\"float64\")\n",
        "walk_noise_y = tf.random.normal(noise.shape, dtype=\"float64\")\n",
        "\n",
        "walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi)\n",
        "walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi)\n",
        "noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0)\n",
        "noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0)\n",
        "noise = ops.add(noise_x, noise_y)\n",
        "batched_noise = ops.split(noise, batches)\n",
        "\n",
        "images = []\n",
        "for batch in range(batches):\n",
        "    images += [\n",
        "        Image.fromarray(img)\n",
        "        for img in stable_diffusion.generate_image(\n",
        "            encoding,\n",
        "            batch_size=batch_size,\n",
        "            num_steps=25,\n",
        "            diffusion_noise=batched_noise[batch],\n",
        "        )\n",
        "    ]\n",
        "\n",
        "gif_maker = []\n",
        "for i in range(0,len(images)):\n",
        "  gif_maker.append(downsample(images[i]))\n",
        "\n",
        "export_as_gif(\"circular_walk_paris_at_night.gif\", gif_maker, frames_per_second=2, rubber_band=True)\n"
      ],
      "metadata": {
        "id": "3EuK29bW1oZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a3659-ddae-46ff-b6b1-943b000eb2d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_diffusion_model.h5\n",
            "3439090152/3439090152 [==============================] - 32s 0us/step\n",
            "25/25 [==============================] - 112s 1s/step\n",
            "Downloading data from https://huggingface.co/fchollet/stable-diffusion/resolve/main/kcv_decoder.h5\n",
            "198180272/198180272 [==============================] - 1s 0us/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 23s 936ms/step\n",
            "25/25 [==============================] - 26s 1s/step\n",
            "25/25 [==============================] - 25s 998ms/step\n",
            "25/25 [==============================] - 24s 979ms/step\n",
            "25/25 [==============================] - 23s 938ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 980ms/step\n",
            "25/25 [==============================] - 24s 974ms/step\n",
            "25/25 [==============================] - 24s 975ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 978ms/step\n",
            "25/25 [==============================] - 26s 1s/step\n",
            "25/25 [==============================] - 25s 1000ms/step\n",
            "25/25 [==============================] - 23s 936ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 976ms/step\n",
            "25/25 [==============================] - 23s 934ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 975ms/step\n",
            "25/25 [==============================] - 26s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 978ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 963ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 23s 935ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 977ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 978ms/step\n",
            "25/25 [==============================] - 24s 977ms/step\n",
            "25/25 [==============================] - 26s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 976ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 977ms/step\n",
            "25/25 [==============================] - 25s 1s/step\n",
            "25/25 [==============================] - 24s 974ms/step\n",
            "25/25 [==============================] - 24s 977ms/step\n",
            "25/25 [==============================] - 24s 975ms/step\n",
            "25/25 [==============================] - 24s 975ms/step\n",
            "25/25 [==============================] - 24s 977ms/step\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1GBTl3mmIAogxA6uf_9Tvk1H2zey7ShMT",
      "authorship_tag": "ABX9TyNkJCZ0/LezZeI1rUkUclud",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}