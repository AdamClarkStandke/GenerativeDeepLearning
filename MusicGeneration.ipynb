{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamClarkStandke/GenerativeDeepLearning/blob/main/MusicGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa3yxsrNzlCI"
      },
      "source": [
        "# Music Generation\n",
        "\n",
        "This code comes from David Foster's [notebook](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/11_music/01_transformer/transformer.ipynb) from his book [Generative Deep Learning](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947).\n",
        "\n",
        "Unlike David Foster's notebook this simple monophonic music generator was trained on the [English Suites](http://www.jsbach.net/midi/midi_english_suites.html) rather than [Suites for Solo Cello](http://www.jsbach.net/midi/midi_solo_cello.html) by [Johann Sebastian Bach](https://en.wikipedia.org/wiki/English_Suites_(Bach). I used to play the piano (thanks to margaret maas the greatest piano teacher ever) and hence that was the reason why I changed the dataset since I do not understand any string type of arraignments ðŸ˜†\n",
        "\n",
        "In David's book [Generative Deep Learning](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947) and [repository] (https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/) he details not only this transformer based music generator but also [MuseGAN](https://github.com/salu133445/musegan) which is able to generate polyphonic compositions.\n",
        "\n",
        "Other than preprocessing  (i.e. converting musical notes and their durations into a computer readable format) and the Sinusoidal positional encoding layer, the model consists of one transformer layer and two output heads of dense form.  \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5P2HWXXhn_Yu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks\n",
        "from fractions import Fraction\n",
        "import music21"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_midi_note(sample_note, sample_duration):\n",
        "    new_note = None\n",
        "\n",
        "    if \"TS\" in sample_note:\n",
        "        new_note = music21.meter.TimeSignature(sample_note.split(\"TS\")[0])\n",
        "\n",
        "    elif \"major\" in sample_note or \"minor\" in sample_note:\n",
        "        tonic, mode = sample_note.split(\":\")\n",
        "        new_note = music21.key.Key(tonic, mode)\n",
        "\n",
        "    elif sample_note == \"rest\":\n",
        "        new_note = music21.note.Rest()\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    elif \".\" in sample_note:\n",
        "        notes_in_chord = sample_note.split(\".\")\n",
        "        chord_notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            n = music21.note.Note(current_note)\n",
        "            n.duration = music21.duration.Duration(\n",
        "                float(Fraction(sample_duration))\n",
        "            )\n",
        "            n.storedInstrument = music21.instrument.Violoncello()\n",
        "            chord_notes.append(n)\n",
        "        new_note = music21.chord.Chord(chord_notes)\n",
        "\n",
        "    elif sample_note == \"rest\":\n",
        "        new_note = music21.note.Rest()\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    elif sample_note != \"START\":\n",
        "        new_note = music21.note.Note(sample_note)\n",
        "        new_note.duration = music21.duration.Duration(\n",
        "            float(Fraction(sample_duration))\n",
        "        )\n",
        "        new_note.storedInstrument = music21.instrument.Violoncello()\n",
        "\n",
        "    return new_note"
      ],
      "metadata": {
        "id": "BJbDL7C7DWsF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gwBKwBHVwCuK"
      },
      "outputs": [],
      "source": [
        "def load_parsed_files_notes(parsed_data_path):\n",
        "    with open(parsed_data_path, \"rb\") as f:\n",
        "        notes = pkl.load(f)\n",
        "    return notes\n",
        "\n",
        "def load_parsed_files_duration(parsed_data_path):\n",
        "    with open(parsed_data_path, \"rb\") as f:\n",
        "        durations = pkl.load(f)\n",
        "    return durations\n",
        "\n",
        "class SinePositionEncoding(keras.layers.Layer):\n",
        "    \"\"\"Sinusoidal positional encoding layer.\n",
        "    This layer calculates the position encoding as a mix of sine and cosine\n",
        "    functions with geometrically increasing wavelengths. Defined and formulized\n",
        "    in [Attention is All You Need](https://arxiv.org/abs/1706.03762).\n",
        "    Takes as input an embedded token tensor. The input must have shape\n",
        "    [batch_size, sequence_length, feature_size]. This layer will return a\n",
        "    positional encoding the same size as the embedded token tensor, which\n",
        "    can be added directly to the embedded token tensor.\n",
        "    Args:\n",
        "        max_wavelength: The maximum angular wavelength of the sine/cosine\n",
        "            curves, as described in Attention is All You Need. Defaults to\n",
        "            10000.\n",
        "    Examples:\n",
        "    ```python\n",
        "    # create a simple embedding layer with sinusoidal positional encoding\n",
        "    seq_len = 100\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 32\n",
        "    inputs = keras.Input((seq_len,), dtype=tf.float32)\n",
        "    embedding = keras.layers.Embedding(\n",
        "        input_dim=vocab_size, output_dim=embedding_dim\n",
        "    )(inputs)\n",
        "    positional_encoding = keras_nlp.layers.SinePositionEncoding()(embedding)\n",
        "    outputs = embedding + positional_encoding\n",
        "    ```\n",
        "    References:\n",
        "     - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        max_wavelength=10000,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_wavelength = max_wavelength\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # TODO(jbischof): replace `hidden_size` with`hidden_dim` for consistency\n",
        "        # with other layers.\n",
        "        input_shape = tf.shape(inputs)\n",
        "        # length of sequence is the second last dimension of the inputs\n",
        "        seq_length = input_shape[-2]\n",
        "        hidden_size = input_shape[-1]\n",
        "        position = tf.cast(tf.range(seq_length), self.compute_dtype)\n",
        "        min_freq = tf.cast(1 / self.max_wavelength, dtype=self.compute_dtype)\n",
        "        timescales = tf.pow(\n",
        "            min_freq,\n",
        "            tf.cast(2 * (tf.range(hidden_size) // 2), self.compute_dtype)\n",
        "            / tf.cast(hidden_size, self.compute_dtype),\n",
        "        )\n",
        "        angles = tf.expand_dims(position, 1) * tf.expand_dims(timescales, 0)\n",
        "        # even indices are sine, odd are cosine\n",
        "        cos_mask = tf.cast(tf.range(hidden_size) % 2, self.compute_dtype)\n",
        "        sin_mask = 1 - cos_mask\n",
        "        # embedding shape is [seq_length, hidden_size]\n",
        "        positional_encodings = (\n",
        "            tf.sin(angles) * sin_mask + tf.cos(angles) * cos_mask\n",
        "        )\n",
        "\n",
        "        return tf.broadcast_to(positional_encodings, input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_wavelength\": self.max_wavelength,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u2F2fKm4w6cQ"
      },
      "outputs": [],
      "source": [
        "PARSE_MIDI_FILES = False\n",
        "PARSED_DATA_PATH = \"/content/drive/MyDrive/music/bach/\"\n",
        "DATASET_REPETITIONS = 1\n",
        "\n",
        "SEQ_LEN = 50\n",
        "EMBEDDING_DIM = 256\n",
        "KEY_DIM = 256\n",
        "N_HEADS = 12\n",
        "DROPOUT_RATE = 0.3\n",
        "FEED_FORWARD_DIM = 256\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# optimization\n",
        "EPOCHS = 200\n",
        "BATCH_SIZE = 500\n",
        "\n",
        "GENERATE_LEN = 77"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataset"
      ],
      "metadata": {
        "id": "eBSr7oajFcMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BqJwZBS8ydYZ"
      },
      "outputs": [],
      "source": [
        "notes = load_parsed_files_notes(\"/content/drive/MyDrive/music/bach/notes\")\n",
        "durations = load_parsed_files_duration(\"/content/drive/MyDrive/music/bach/durations\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_tECR8gr4W5L"
      },
      "outputs": [],
      "source": [
        "def create_dataset(elements):\n",
        "    ds = (\n",
        "        tf.data.Dataset.from_tensor_slices(elements)\n",
        "        .batch(BATCH_SIZE, drop_remainder=True)\n",
        "        .shuffle(1000)\n",
        "    )\n",
        "    vectorize_layer = layers.TextVectorization(\n",
        "        standardize=None, output_mode=\"int\"\n",
        "    )\n",
        "    vectorize_layer.adapt(ds)\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    return ds, vectorize_layer, vocab\n",
        "\n",
        "\n",
        "notes_seq_ds, notes_vectorize_layer, notes_vocab = create_dataset(notes)\n",
        "durations_seq_ds, durations_vectorize_layer, durations_vocab = create_dataset(\n",
        "    durations\n",
        ")\n",
        "seq_ds = tf.data.Dataset.zip((notes_seq_ds, durations_seq_ds))\n",
        "notes_vocab_size = len(notes_vocab)\n",
        "durations_vocab_size = len(durations_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7fNLbqhp474K"
      },
      "outputs": [],
      "source": [
        "# Create the training set of sequences and the same sequences shifted by one note\n",
        "def prepare_inputs(notes, durations):\n",
        "    notes = tf.expand_dims(notes, -1)\n",
        "    durations = tf.expand_dims(durations, -1)\n",
        "    tokenized_notes = notes_vectorize_layer(notes)\n",
        "    tokenized_durations = durations_vectorize_layer(durations)\n",
        "    x = (tokenized_notes[:, :-1], tokenized_durations[:, :-1])\n",
        "    y = (tokenized_notes[:, 1:], tokenized_durations[:, 1:])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "ds = seq_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Model"
      ],
      "metadata": {
        "id": "n6Tc2NhfFXzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t0ZXRGf35Qjl"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mLQsAQX85gCo"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        key_dim,\n",
        "        embed_dim,\n",
        "        ff_dim,\n",
        "        name,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "    ):\n",
        "        super(TransformerBlock, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.attn = layers.MultiHeadAttention(\n",
        "            num_heads, key_dim, output_shape=embed_dim\n",
        "        )\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(\n",
        "            batch_size, seq_len, seq_len, tf.bool\n",
        "        )\n",
        "        attention_output, attention_scores = self.attn(\n",
        "            inputs,\n",
        "            inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q-Bsq1EF52DH"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embed_dim,\n",
        "            embeddings_initializer=\"he_uniform\",\n",
        "        )\n",
        "        self.pos_emb = SinePositionEncoding()\n",
        "\n",
        "    def call(self, x):\n",
        "        embedding = self.token_emb(x)\n",
        "        positions = self.pos_emb(embedding)\n",
        "        return embedding + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ritiELH59LL"
      },
      "outputs": [],
      "source": [
        "note_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "durations_inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "note_embeddings = TokenAndPositionEmbedding(\n",
        "    notes_vocab_size, EMBEDDING_DIM // 2\n",
        ")(note_inputs)\n",
        "duration_embeddings = TokenAndPositionEmbedding(\n",
        "    durations_vocab_size, EMBEDDING_DIM // 2\n",
        ")(durations_inputs)\n",
        "embeddings = layers.Concatenate()([note_embeddings, duration_embeddings])\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM, name=\"attention\"\n",
        ")(embeddings)\n",
        "note_outputs = layers.Dense(\n",
        "    notes_vocab_size, activation=\"softmax\", name=\"note_outputs\"\n",
        ")(x)\n",
        "duration_outputs = layers.Dense(\n",
        "    durations_vocab_size, activation=\"softmax\", name=\"duration_outputs\"\n",
        ")(x)\n",
        "model = models.Model(\n",
        "    inputs=[note_inputs, durations_inputs],\n",
        "    outputs=[note_outputs, duration_outputs],  # attention_scores\n",
        ")\n",
        "model.compile(\n",
        "    \"adam\",\n",
        "    loss=[\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "        losses.SparseCategoricalCrossentropy(),\n",
        "    ],\n",
        ")\n",
        "att_model = models.Model(\n",
        "    inputs=[note_inputs, durations_inputs], outputs=attention_scores\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "byJxF7G47HW5"
      },
      "outputs": [],
      "source": [
        "# Create a MusicGenerator checkpoint\n",
        "class MusicGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_note, index_to_duration, top_k=10):\n",
        "        self.index_to_note = index_to_note\n",
        "        self.note_to_index = {\n",
        "            note: index for index, note in enumerate(index_to_note)\n",
        "        }\n",
        "        self.index_to_duration = index_to_duration\n",
        "        self.duration_to_index = {\n",
        "            duration: index for index, duration in enumerate(index_to_duration)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def get_note(self, notes, durations, temperature):\n",
        "        sample_note_idx = 1\n",
        "        while sample_note_idx == 1:\n",
        "            sample_note_idx, note_probs = self.sample_from(\n",
        "                notes[0][-1], temperature\n",
        "            )\n",
        "            sample_note = self.index_to_note[sample_note_idx]\n",
        "\n",
        "        sample_duration_idx = 1\n",
        "        while sample_duration_idx == 1:\n",
        "            sample_duration_idx, duration_probs = self.sample_from(\n",
        "                durations[0][-1], temperature\n",
        "            )\n",
        "            sample_duration = self.index_to_duration[sample_duration_idx]\n",
        "\n",
        "        new_note = get_midi_note(sample_note, sample_duration)\n",
        "\n",
        "        return (\n",
        "            new_note,\n",
        "            sample_note_idx,\n",
        "            sample_note,\n",
        "            note_probs,\n",
        "            sample_duration_idx,\n",
        "            sample_duration,\n",
        "            duration_probs,\n",
        "        )\n",
        "\n",
        "    def generate(self, start_notes, start_durations, max_tokens, temperature):\n",
        "        attention_model = models.Model(\n",
        "            inputs=self.model.input,\n",
        "            outputs=self.model.get_layer(\"attention\").output,\n",
        "        )\n",
        "\n",
        "        start_note_tokens = [self.note_to_index.get(x, 1) for x in start_notes]\n",
        "        start_duration_tokens = [\n",
        "            self.duration_to_index.get(x, 1) for x in start_durations\n",
        "        ]\n",
        "        sample_note = None\n",
        "        sample_duration = None\n",
        "        info = []\n",
        "        midi_stream = music21.stream.Stream()\n",
        "\n",
        "        midi_stream.append(music21.clef.BassClef())\n",
        "\n",
        "        for sample_note, sample_duration in zip(start_notes, start_durations):\n",
        "            new_note = get_midi_note(sample_note, sample_duration)\n",
        "            if new_note is not None:\n",
        "                midi_stream.append(new_note)\n",
        "\n",
        "        while len(start_note_tokens) < max_tokens:\n",
        "            x1 = np.array([start_note_tokens])\n",
        "            x2 = np.array([start_duration_tokens])\n",
        "            notes, durations = self.model.predict([x1, x2], verbose=0)\n",
        "\n",
        "            repeat = True\n",
        "\n",
        "            while repeat:\n",
        "                (\n",
        "                    new_note,\n",
        "                    sample_note_idx,\n",
        "                    sample_note,\n",
        "                    note_probs,\n",
        "                    sample_duration_idx,\n",
        "                    sample_duration,\n",
        "                    duration_probs,\n",
        "                ) = self.get_note(notes, durations, temperature)\n",
        "\n",
        "                if (\n",
        "                    isinstance(new_note, music21.chord.Chord)\n",
        "                    or isinstance(new_note, music21.note.Note)\n",
        "                    or isinstance(new_note, music21.note.Rest)\n",
        "                ) and sample_duration == \"0.0\":\n",
        "                    repeat = True\n",
        "                else:\n",
        "                    repeat = False\n",
        "\n",
        "            if new_note is not None:\n",
        "                midi_stream.append(new_note)\n",
        "\n",
        "            _, att = attention_model.predict([x1, x2], verbose=0)\n",
        "\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": [start_notes.copy(), start_durations.copy()],\n",
        "                    \"midi\": midi_stream,\n",
        "                    \"chosen_note\": (sample_note, sample_duration),\n",
        "                    \"note_probs\": note_probs,\n",
        "                    \"duration_probs\": duration_probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "            start_note_tokens.append(sample_note_idx)\n",
        "            start_duration_tokens.append(sample_duration_idx)\n",
        "            start_notes.append(sample_note)\n",
        "            start_durations.append(sample_duration)\n",
        "\n",
        "            if sample_note == \"START\":\n",
        "                break\n",
        "\n",
        "        return info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "NT2nF73bFT8K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilUiA_Np78P9",
        "outputId": "ecc8f3e3-931a-4ce5-f7da-8e23d02f7656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            " 6/88 [=>............................] - ETA: 44s - loss: 6.6930 - note_outputs_loss: 4.3098 - duration_outputs_loss: 2.3832"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2135s vs `on_train_batch_end` time: 0.3727s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 24s 192ms/step - loss: 4.8459 - note_outputs_loss: 3.5727 - duration_outputs_loss: 1.2732\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 4.3706 - note_outputs_loss: 3.3182 - duration_outputs_loss: 1.0524\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 3.7023 - note_outputs_loss: 2.8127 - duration_outputs_loss: 0.8897\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 3.4722 - note_outputs_loss: 2.6201 - duration_outputs_loss: 0.8521\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 3.3426 - note_outputs_loss: 2.5343 - duration_outputs_loss: 0.8084\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 3.2710 - note_outputs_loss: 2.4787 - duration_outputs_loss: 0.7923\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 14s 158ms/step - loss: 3.2248 - note_outputs_loss: 2.4377 - duration_outputs_loss: 0.7871\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 15s 171ms/step - loss: 3.1771 - note_outputs_loss: 2.3975 - duration_outputs_loss: 0.7796\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 13s 142ms/step - loss: 3.1408 - note_outputs_loss: 2.3754 - duration_outputs_loss: 0.7654\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 13s 142ms/step - loss: 3.1049 - note_outputs_loss: 2.3439 - duration_outputs_loss: 0.7610\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 15s 171ms/step - loss: 3.0797 - note_outputs_loss: 2.3280 - duration_outputs_loss: 0.7517\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 13s 146ms/step - loss: 3.0641 - note_outputs_loss: 2.3147 - duration_outputs_loss: 0.7494\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 3.0481 - note_outputs_loss: 2.2965 - duration_outputs_loss: 0.7516\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 3.0236 - note_outputs_loss: 2.2825 - duration_outputs_loss: 0.7410\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 2.9986 - note_outputs_loss: 2.2595 - duration_outputs_loss: 0.7391\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 2.9896 - note_outputs_loss: 2.2546 - duration_outputs_loss: 0.7350\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 13s 144ms/step - loss: 2.9664 - note_outputs_loss: 2.2340 - duration_outputs_loss: 0.7323\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 2.9565 - note_outputs_loss: 2.2258 - duration_outputs_loss: 0.7307\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 12s 126ms/step - loss: 2.9258 - note_outputs_loss: 2.2102 - duration_outputs_loss: 0.7156\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 2.9098 - note_outputs_loss: 2.1976 - duration_outputs_loss: 0.7122\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 11s 124ms/step - loss: 2.8891 - note_outputs_loss: 2.1835 - duration_outputs_loss: 0.7055\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 10s 118ms/step - loss: 2.8806 - note_outputs_loss: 2.1707 - duration_outputs_loss: 0.7099\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 13s 152ms/step - loss: 2.8641 - note_outputs_loss: 2.1576 - duration_outputs_loss: 0.7065\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 9s 105ms/step - loss: 2.8467 - note_outputs_loss: 2.1465 - duration_outputs_loss: 0.7002\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 2.8296 - note_outputs_loss: 2.1325 - duration_outputs_loss: 0.6972\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 13s 146ms/step - loss: 2.8263 - note_outputs_loss: 2.1231 - duration_outputs_loss: 0.7032\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 10s 116ms/step - loss: 2.8102 - note_outputs_loss: 2.1145 - duration_outputs_loss: 0.6957\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 14s 157ms/step - loss: 2.8043 - note_outputs_loss: 2.1049 - duration_outputs_loss: 0.6994\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 2.7836 - note_outputs_loss: 2.0928 - duration_outputs_loss: 0.6908\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 13s 145ms/step - loss: 2.7622 - note_outputs_loss: 2.0747 - duration_outputs_loss: 0.6875\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 2.7577 - note_outputs_loss: 2.0655 - duration_outputs_loss: 0.6922\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 13s 153ms/step - loss: 2.7504 - note_outputs_loss: 2.0611 - duration_outputs_loss: 0.6893\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 11s 121ms/step - loss: 2.7239 - note_outputs_loss: 2.0430 - duration_outputs_loss: 0.6809\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 2.7182 - note_outputs_loss: 2.0379 - duration_outputs_loss: 0.6803\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 14s 159ms/step - loss: 2.7038 - note_outputs_loss: 2.0207 - duration_outputs_loss: 0.6831\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 2.7052 - note_outputs_loss: 2.0158 - duration_outputs_loss: 0.6894\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 2.6800 - note_outputs_loss: 1.9977 - duration_outputs_loss: 0.6823\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 2.6640 - note_outputs_loss: 1.9848 - duration_outputs_loss: 0.6792\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 12s 141ms/step - loss: 2.6528 - note_outputs_loss: 1.9733 - duration_outputs_loss: 0.6795\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 2.6310 - note_outputs_loss: 1.9546 - duration_outputs_loss: 0.6764\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 2.6218 - note_outputs_loss: 1.9492 - duration_outputs_loss: 0.6726\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 2.6111 - note_outputs_loss: 1.9362 - duration_outputs_loss: 0.6750\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 15s 175ms/step - loss: 2.5953 - note_outputs_loss: 1.9205 - duration_outputs_loss: 0.6748\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 15s 171ms/step - loss: 2.6024 - note_outputs_loss: 1.9243 - duration_outputs_loss: 0.6781\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 2.5814 - note_outputs_loss: 1.9072 - duration_outputs_loss: 0.6741\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 10s 117ms/step - loss: 2.5648 - note_outputs_loss: 1.8900 - duration_outputs_loss: 0.6748\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 2.5481 - note_outputs_loss: 1.8717 - duration_outputs_loss: 0.6764\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 13s 145ms/step - loss: 2.5371 - note_outputs_loss: 1.8621 - duration_outputs_loss: 0.6750\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 2.5383 - note_outputs_loss: 1.8592 - duration_outputs_loss: 0.6790\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 2.5181 - note_outputs_loss: 1.8442 - duration_outputs_loss: 0.6738\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 2.4999 - note_outputs_loss: 1.8299 - duration_outputs_loss: 0.6701\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 11s 128ms/step - loss: 2.5002 - note_outputs_loss: 1.8266 - duration_outputs_loss: 0.6736\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 2.4725 - note_outputs_loss: 1.8045 - duration_outputs_loss: 0.6681\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 2.4605 - note_outputs_loss: 1.7931 - duration_outputs_loss: 0.6674\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 2.4602 - note_outputs_loss: 1.7914 - duration_outputs_loss: 0.6687\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 13s 145ms/step - loss: 2.4423 - note_outputs_loss: 1.7740 - duration_outputs_loss: 0.6683\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 16s 177ms/step - loss: 2.4256 - note_outputs_loss: 1.7596 - duration_outputs_loss: 0.6660\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 14s 157ms/step - loss: 2.4130 - note_outputs_loss: 1.7460 - duration_outputs_loss: 0.6670\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 2.4057 - note_outputs_loss: 1.7356 - duration_outputs_loss: 0.6701\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 11s 125ms/step - loss: 2.4001 - note_outputs_loss: 1.7244 - duration_outputs_loss: 0.6757\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 2.3851 - note_outputs_loss: 1.7184 - duration_outputs_loss: 0.6667\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 2.3705 - note_outputs_loss: 1.7056 - duration_outputs_loss: 0.6649\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 2.3555 - note_outputs_loss: 1.6923 - duration_outputs_loss: 0.6633\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 2.3514 - note_outputs_loss: 1.6882 - duration_outputs_loss: 0.6632\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 15s 167ms/step - loss: 2.3297 - note_outputs_loss: 1.6668 - duration_outputs_loss: 0.6629\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 13s 150ms/step - loss: 2.3258 - note_outputs_loss: 1.6633 - duration_outputs_loss: 0.6624\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 11s 119ms/step - loss: 2.3180 - note_outputs_loss: 1.6574 - duration_outputs_loss: 0.6605\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 2.3109 - note_outputs_loss: 1.6502 - duration_outputs_loss: 0.6607\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 13s 144ms/step - loss: 2.2957 - note_outputs_loss: 1.6344 - duration_outputs_loss: 0.6613\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 12s 132ms/step - loss: 2.2834 - note_outputs_loss: 1.6218 - duration_outputs_loss: 0.6617\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 2.2724 - note_outputs_loss: 1.6144 - duration_outputs_loss: 0.6579\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 2.2775 - note_outputs_loss: 1.6169 - duration_outputs_loss: 0.6606\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 2.2603 - note_outputs_loss: 1.5965 - duration_outputs_loss: 0.6639\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 13s 149ms/step - loss: 2.2537 - note_outputs_loss: 1.5930 - duration_outputs_loss: 0.6607\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 2.2362 - note_outputs_loss: 1.5787 - duration_outputs_loss: 0.6575\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 17s 195ms/step - loss: 2.2299 - note_outputs_loss: 1.5732 - duration_outputs_loss: 0.6567\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 15s 170ms/step - loss: 2.2170 - note_outputs_loss: 1.5623 - duration_outputs_loss: 0.6547\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 14s 148ms/step - loss: 2.2120 - note_outputs_loss: 1.5532 - duration_outputs_loss: 0.6589\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 2.2003 - note_outputs_loss: 1.5459 - duration_outputs_loss: 0.6544\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 16s 172ms/step - loss: 2.1985 - note_outputs_loss: 1.5405 - duration_outputs_loss: 0.6579\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 2.1898 - note_outputs_loss: 1.5313 - duration_outputs_loss: 0.6586\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 2.1769 - note_outputs_loss: 1.5237 - duration_outputs_loss: 0.6532\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 13s 150ms/step - loss: 2.1727 - note_outputs_loss: 1.5149 - duration_outputs_loss: 0.6579\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 2.1655 - note_outputs_loss: 1.5066 - duration_outputs_loss: 0.6589\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 12s 124ms/step - loss: 2.1634 - note_outputs_loss: 1.5041 - duration_outputs_loss: 0.6593\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 14s 159ms/step - loss: 2.1513 - note_outputs_loss: 1.4952 - duration_outputs_loss: 0.6561\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 2.1502 - note_outputs_loss: 1.4892 - duration_outputs_loss: 0.6610\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 2.1263 - note_outputs_loss: 1.4737 - duration_outputs_loss: 0.6526\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 2.1317 - note_outputs_loss: 1.4727 - duration_outputs_loss: 0.6589\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 13s 152ms/step - loss: 2.1262 - note_outputs_loss: 1.4698 - duration_outputs_loss: 0.6564\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 2.1314 - note_outputs_loss: 1.4676 - duration_outputs_loss: 0.6639\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 12s 142ms/step - loss: 2.1117 - note_outputs_loss: 1.4570 - duration_outputs_loss: 0.6547\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 15s 174ms/step - loss: 2.0977 - note_outputs_loss: 1.4457 - duration_outputs_loss: 0.6521\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 2.1096 - note_outputs_loss: 1.4524 - duration_outputs_loss: 0.6572\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 2.0995 - note_outputs_loss: 1.4438 - duration_outputs_loss: 0.6557\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 2.0850 - note_outputs_loss: 1.4289 - duration_outputs_loss: 0.6561\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 2.0801 - note_outputs_loss: 1.4225 - duration_outputs_loss: 0.6576\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 2.0811 - note_outputs_loss: 1.4227 - duration_outputs_loss: 0.6584\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 13s 148ms/step - loss: 2.0747 - note_outputs_loss: 1.4164 - duration_outputs_loss: 0.6583\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 2.0675 - note_outputs_loss: 1.4111 - duration_outputs_loss: 0.6565\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 16s 181ms/step - loss: 2.0620 - note_outputs_loss: 1.4046 - duration_outputs_loss: 0.6574\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 2.0521 - note_outputs_loss: 1.3959 - duration_outputs_loss: 0.6562\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 2.0528 - note_outputs_loss: 1.3929 - duration_outputs_loss: 0.6598\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 14s 165ms/step - loss: 2.0487 - note_outputs_loss: 1.3933 - duration_outputs_loss: 0.6554\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 2.0425 - note_outputs_loss: 1.3840 - duration_outputs_loss: 0.6584\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 2.0330 - note_outputs_loss: 1.3741 - duration_outputs_loss: 0.6589\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 11s 129ms/step - loss: 2.0344 - note_outputs_loss: 1.3750 - duration_outputs_loss: 0.6594\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 13s 143ms/step - loss: 2.0280 - note_outputs_loss: 1.3718 - duration_outputs_loss: 0.6562\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 2.0241 - note_outputs_loss: 1.3650 - duration_outputs_loss: 0.6591\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 2.0101 - note_outputs_loss: 1.3541 - duration_outputs_loss: 0.6560\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 14s 157ms/step - loss: 2.0189 - note_outputs_loss: 1.3592 - duration_outputs_loss: 0.6597\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 10s 112ms/step - loss: 2.0098 - note_outputs_loss: 1.3521 - duration_outputs_loss: 0.6577\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 11s 124ms/step - loss: 2.0093 - note_outputs_loss: 1.3502 - duration_outputs_loss: 0.6591\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 13s 146ms/step - loss: 2.0024 - note_outputs_loss: 1.3427 - duration_outputs_loss: 0.6597\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 13s 149ms/step - loss: 1.9942 - note_outputs_loss: 1.3370 - duration_outputs_loss: 0.6572\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 1.9884 - note_outputs_loss: 1.3312 - duration_outputs_loss: 0.6571\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 1.9902 - note_outputs_loss: 1.3316 - duration_outputs_loss: 0.6586\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 12s 141ms/step - loss: 1.9879 - note_outputs_loss: 1.3283 - duration_outputs_loss: 0.6595\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 1.9777 - note_outputs_loss: 1.3202 - duration_outputs_loss: 0.6575\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 14s 164ms/step - loss: 1.9770 - note_outputs_loss: 1.3220 - duration_outputs_loss: 0.6550\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 1.9747 - note_outputs_loss: 1.3169 - duration_outputs_loss: 0.6578\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 16s 177ms/step - loss: 1.9701 - note_outputs_loss: 1.3111 - duration_outputs_loss: 0.6591\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 15s 171ms/step - loss: 1.9618 - note_outputs_loss: 1.3058 - duration_outputs_loss: 0.6561\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 1.9582 - note_outputs_loss: 1.2980 - duration_outputs_loss: 0.6602\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 1.9515 - note_outputs_loss: 1.2959 - duration_outputs_loss: 0.6556\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 11s 121ms/step - loss: 1.9559 - note_outputs_loss: 1.2960 - duration_outputs_loss: 0.6598\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 1.9367 - note_outputs_loss: 1.2823 - duration_outputs_loss: 0.6545\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 1.9442 - note_outputs_loss: 1.2864 - duration_outputs_loss: 0.6579\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 1.9456 - note_outputs_loss: 1.2891 - duration_outputs_loss: 0.6565\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 13s 150ms/step - loss: 1.9376 - note_outputs_loss: 1.2820 - duration_outputs_loss: 0.6556\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 13s 149ms/step - loss: 1.9366 - note_outputs_loss: 1.2775 - duration_outputs_loss: 0.6591\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 15s 165ms/step - loss: 1.9296 - note_outputs_loss: 1.2744 - duration_outputs_loss: 0.6551\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 1.9238 - note_outputs_loss: 1.2671 - duration_outputs_loss: 0.6568\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 1.9211 - note_outputs_loss: 1.2680 - duration_outputs_loss: 0.6531\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 1.9194 - note_outputs_loss: 1.2641 - duration_outputs_loss: 0.6553\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 9s 107ms/step - loss: 1.9195 - note_outputs_loss: 1.2646 - duration_outputs_loss: 0.6549\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 1.9158 - note_outputs_loss: 1.2607 - duration_outputs_loss: 0.6551\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 13s 148ms/step - loss: 1.9045 - note_outputs_loss: 1.2507 - duration_outputs_loss: 0.6538\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 15s 173ms/step - loss: 1.9104 - note_outputs_loss: 1.2519 - duration_outputs_loss: 0.6585\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 17s 196ms/step - loss: 1.9019 - note_outputs_loss: 1.2468 - duration_outputs_loss: 0.6551\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 1.9065 - note_outputs_loss: 1.2507 - duration_outputs_loss: 0.6558\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 13s 144ms/step - loss: 1.8930 - note_outputs_loss: 1.2384 - duration_outputs_loss: 0.6546\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 1.8962 - note_outputs_loss: 1.2398 - duration_outputs_loss: 0.6564\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 1.8920 - note_outputs_loss: 1.2385 - duration_outputs_loss: 0.6535\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 13s 150ms/step - loss: 1.8879 - note_outputs_loss: 1.2334 - duration_outputs_loss: 0.6545\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 14s 157ms/step - loss: 1.8878 - note_outputs_loss: 1.2319 - duration_outputs_loss: 0.6559\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 13s 142ms/step - loss: 1.8796 - note_outputs_loss: 1.2239 - duration_outputs_loss: 0.6558\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 11s 125ms/step - loss: 1.8764 - note_outputs_loss: 1.2214 - duration_outputs_loss: 0.6550\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 1.8775 - note_outputs_loss: 1.2238 - duration_outputs_loss: 0.6536\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 11s 121ms/step - loss: 1.8844 - note_outputs_loss: 1.2316 - duration_outputs_loss: 0.6528\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 1.8712 - note_outputs_loss: 1.2167 - duration_outputs_loss: 0.6545\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 1.8710 - note_outputs_loss: 1.2135 - duration_outputs_loss: 0.6575\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 12s 131ms/step - loss: 1.8651 - note_outputs_loss: 1.2112 - duration_outputs_loss: 0.6538\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 1.8633 - note_outputs_loss: 1.2077 - duration_outputs_loss: 0.6555\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 14s 160ms/step - loss: 1.8583 - note_outputs_loss: 1.2062 - duration_outputs_loss: 0.6521\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 1.8564 - note_outputs_loss: 1.2025 - duration_outputs_loss: 0.6538\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 1.8499 - note_outputs_loss: 1.1980 - duration_outputs_loss: 0.6520\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 12s 133ms/step - loss: 1.8637 - note_outputs_loss: 1.2064 - duration_outputs_loss: 0.6573\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 11s 130ms/step - loss: 1.8569 - note_outputs_loss: 1.1987 - duration_outputs_loss: 0.6581\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 1.8470 - note_outputs_loss: 1.1952 - duration_outputs_loss: 0.6518\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 1.8433 - note_outputs_loss: 1.1895 - duration_outputs_loss: 0.6538\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 1.8393 - note_outputs_loss: 1.1860 - duration_outputs_loss: 0.6532\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 13s 151ms/step - loss: 1.8355 - note_outputs_loss: 1.1819 - duration_outputs_loss: 0.6536\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 15s 170ms/step - loss: 1.8368 - note_outputs_loss: 1.1826 - duration_outputs_loss: 0.6542\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 12s 138ms/step - loss: 1.8343 - note_outputs_loss: 1.1798 - duration_outputs_loss: 0.6545\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 1.8321 - note_outputs_loss: 1.1778 - duration_outputs_loss: 0.6543\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 1.8295 - note_outputs_loss: 1.1781 - duration_outputs_loss: 0.6514\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 13s 148ms/step - loss: 1.8300 - note_outputs_loss: 1.1759 - duration_outputs_loss: 0.6540\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 1.8194 - note_outputs_loss: 1.1674 - duration_outputs_loss: 0.6520\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 14s 162ms/step - loss: 1.8217 - note_outputs_loss: 1.1682 - duration_outputs_loss: 0.6535\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 15s 169ms/step - loss: 1.8286 - note_outputs_loss: 1.1721 - duration_outputs_loss: 0.6564\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 14s 161ms/step - loss: 1.8175 - note_outputs_loss: 1.1631 - duration_outputs_loss: 0.6545\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 14s 154ms/step - loss: 1.8254 - note_outputs_loss: 1.1669 - duration_outputs_loss: 0.6585\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 14s 155ms/step - loss: 1.8135 - note_outputs_loss: 1.1609 - duration_outputs_loss: 0.6526\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 12s 136ms/step - loss: 1.8125 - note_outputs_loss: 1.1598 - duration_outputs_loss: 0.6527\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 15s 166ms/step - loss: 1.8163 - note_outputs_loss: 1.1558 - duration_outputs_loss: 0.6605\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 11s 120ms/step - loss: 1.8173 - note_outputs_loss: 1.1616 - duration_outputs_loss: 0.6557\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 13s 144ms/step - loss: 1.8041 - note_outputs_loss: 1.1515 - duration_outputs_loss: 0.6526\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 13s 146ms/step - loss: 1.8065 - note_outputs_loss: 1.1506 - duration_outputs_loss: 0.6560\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 13s 146ms/step - loss: 1.7989 - note_outputs_loss: 1.1470 - duration_outputs_loss: 0.6519\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 11s 127ms/step - loss: 1.7988 - note_outputs_loss: 1.1465 - duration_outputs_loss: 0.6522\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 10s 113ms/step - loss: 1.8034 - note_outputs_loss: 1.1466 - duration_outputs_loss: 0.6569\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 14s 156ms/step - loss: 1.8017 - note_outputs_loss: 1.1480 - duration_outputs_loss: 0.6537\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 1.7981 - note_outputs_loss: 1.1436 - duration_outputs_loss: 0.6546\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 12s 137ms/step - loss: 1.7910 - note_outputs_loss: 1.1386 - duration_outputs_loss: 0.6524\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 12s 134ms/step - loss: 1.7888 - note_outputs_loss: 1.1358 - duration_outputs_loss: 0.6530\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 13s 147ms/step - loss: 1.7899 - note_outputs_loss: 1.1351 - duration_outputs_loss: 0.6548\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 12s 142ms/step - loss: 1.7884 - note_outputs_loss: 1.1359 - duration_outputs_loss: 0.6525\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 14s 163ms/step - loss: 1.7844 - note_outputs_loss: 1.1308 - duration_outputs_loss: 0.6536\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 1.7832 - note_outputs_loss: 1.1266 - duration_outputs_loss: 0.6566\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 14s 156ms/step - loss: 1.7864 - note_outputs_loss: 1.1278 - duration_outputs_loss: 0.6586\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 14s 158ms/step - loss: 1.7798 - note_outputs_loss: 1.1248 - duration_outputs_loss: 0.6549\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 10s 119ms/step - loss: 1.7798 - note_outputs_loss: 1.1275 - duration_outputs_loss: 0.6523\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 11s 124ms/step - loss: 1.7735 - note_outputs_loss: 1.1207 - duration_outputs_loss: 0.6528\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 12s 140ms/step - loss: 1.7789 - note_outputs_loss: 1.1242 - duration_outputs_loss: 0.6547\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 15s 170ms/step - loss: 1.7760 - note_outputs_loss: 1.1239 - duration_outputs_loss: 0.6522\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 12s 135ms/step - loss: 1.7669 - note_outputs_loss: 1.1130 - duration_outputs_loss: 0.6539\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 12s 139ms/step - loss: 1.7690 - note_outputs_loss: 1.1153 - duration_outputs_loss: 0.6537\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 14s 156ms/step - loss: 1.7671 - note_outputs_loss: 1.1106 - duration_outputs_loss: 0.6565\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 10s 116ms/step - loss: 1.7707 - note_outputs_loss: 1.1143 - duration_outputs_loss: 0.6564\n"
          ]
        }
      ],
      "source": [
        "# Tokenize starting prompt\n",
        "music_generator = MusicGenerator(notes_vocab, durations_vocab)\n",
        "\n",
        "model.fit(\n",
        "    ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        music_generator,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "model.save(\"/content/drive/MyDrive/music/Transformer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Music"
      ],
      "metadata": {
        "id": "4Ld6cw2hFJ0m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jK8yPsQYDNRW"
      },
      "outputs": [],
      "source": [
        "#generating music with temperature 0.5\n",
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens=50, temperature=0.5\n",
        ")\n",
        "midi_stream = info[-1][\"midi\"].chordify()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "midi_stream.show('text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ID0H6UOSDfWF",
        "outputId": "7fdd1182-5c2b-4b44-92fb-5ff95d808242"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0} <music21.clef.BassClef>\n",
            "{0.0} <music21.key.Key of C major>\n",
            "{0.0} <music21.meter.TimeSignature 1/8>\n",
            "{0.0} <music21.chord.Chord A5>\n",
            "{0.5} <music21.meter.TimeSignature 3/2>\n",
            "{0.5} <music21.chord.Chord A5>\n",
            "{1.0} <music21.chord.Chord A5>\n",
            "{1.5} <music21.chord.Chord A5>\n",
            "{2.0} <music21.chord.Chord A5>\n",
            "{2.5} <music21.chord.Chord E5>\n",
            "{3.0} <music21.chord.Chord F#5>\n",
            "{3.5} <music21.chord.Chord F#5>\n",
            "{4.0} <music21.chord.Chord E-5>\n",
            "{4.5} <music21.chord.Chord E5>\n",
            "{5.0} <music21.chord.Chord F#5>\n",
            "{5.5} <music21.chord.Chord G5>\n",
            "{6.0} <music21.chord.Chord A5>\n",
            "{6.5} <music21.chord.Chord B5>\n",
            "{7.0} <music21.chord.Chord C6>\n",
            "{7.5} <music21.chord.Chord C6>\n",
            "{8.0} <music21.chord.Chord B5>\n",
            "{8.5} <music21.chord.Chord A5>\n",
            "{9.0} <music21.chord.Chord G#5>\n",
            "{9.5} <music21.chord.Chord F#5>\n",
            "{10.0} <music21.chord.Chord G#5>\n",
            "{10.5} <music21.chord.Chord B5>\n",
            "{11.0} <music21.chord.Chord A5>\n",
            "{11.5} <music21.chord.Chord G#5>\n",
            "{12.0} <music21.chord.Chord A5>\n",
            "{12.5} <music21.chord.Chord C6>\n",
            "{13.0} <music21.chord.Chord G#5>\n",
            "{13.25} <music21.chord.Chord F#5>\n",
            "{13.5} <music21.chord.Chord G#5>\n",
            "{14.0} <music21.chord.Chord B5>\n",
            "{14.5} <music21.chord.Chord F#5>\n",
            "{15.0} <music21.chord.Chord E5>\n",
            "{15.5} <music21.chord.Chord F#5>\n",
            "{16.0} <music21.chord.Chord G5>\n",
            "{16.5} <music21.chord.Chord G5>\n",
            "{16.6667} <music21.chord.Chord B3>\n",
            "{17.0} <music21.chord.Chord A4>\n",
            "{17.5} <music21.chord.Chord G#4>\n",
            "{18.0} <music21.chord.Chord F#5>\n",
            "{18.5} <music21.chord.Chord B4>\n",
            "{19.0} <music21.chord.Chord E5>\n",
            "{19.5} <music21.chord.Chord E5>\n",
            "{20.0} <music21.chord.Chord E5>\n",
            "{20.5} <music21.chord.Chord E5>\n",
            "{21.0} <music21.chord.Chord E5>\n",
            "{21.5} <music21.chord.Chord E5>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating music with temperature 1\n",
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens=50, temperature=1\n",
        ")\n",
        "midi_stream_two = info[-1][\"midi\"].chordify()\n",
        "midi_stream_two.show('text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWC0CUd6EUvk",
        "outputId": "42ffeec5-2c0a-4bf1-9e20-427485171867"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0} <music21.clef.BassClef>\n",
            "{0.0} <music21.meter.TimeSignature 3/4>\n",
            "{0.0} <music21.chord.Chord D5>\n",
            "{0.25} <music21.chord.Chord E-5>\n",
            "{0.5} <music21.chord.Chord E-5>\n",
            "{1.0} <music21.note.Rest eighth>\n",
            "{1.5} <music21.chord.Chord D5>\n",
            "{2.0} <music21.chord.Chord C5>\n",
            "{2.5} <music21.chord.Chord C5>\n",
            "{2.75} <music21.chord.Chord D5>\n",
            "{3.0} <music21.note.Rest eighth>\n",
            "{3.5} <music21.chord.Chord G4>\n",
            "{4.0} <music21.chord.Chord A4>\n",
            "{4.5} <music21.chord.Chord A4>\n",
            "{5.0} <music21.chord.Chord A4>\n",
            "{5.25} <music21.chord.Chord B-4>\n",
            "{5.5} <music21.chord.Chord B-4>\n",
            "{6.0} <music21.chord.Chord C5>\n",
            "{6.5} <music21.chord.Chord C5>\n",
            "{7.0} <music21.chord.Chord C5>\n",
            "{7.5} <music21.chord.Chord A4>\n",
            "{7.75} <music21.chord.Chord A4>\n",
            "{8.0} <music21.chord.Chord F4>\n",
            "{8.5} <music21.chord.Chord C5>\n",
            "{9.0} <music21.chord.Chord B-4>\n",
            "{9.25} <music21.chord.Chord A4>\n",
            "{9.5} <music21.chord.Chord G4>\n",
            "{10.0} <music21.chord.Chord C5>\n",
            "{10.5} <music21.chord.Chord C5>\n",
            "{11.0} <music21.chord.Chord B-4>\n",
            "{11.5} <music21.chord.Chord B-4>\n",
            "{12.0} <music21.chord.Chord C5>\n",
            "{12.5} <music21.chord.Chord C5>\n",
            "{13.0} <music21.chord.Chord D5>\n",
            "{13.5} <music21.chord.Chord C5>\n",
            "{14.0} <music21.chord.Chord C5>\n",
            "{14.5} <music21.chord.Chord E-5>\n",
            "{15.0} <music21.chord.Chord D5>\n",
            "{15.5} <music21.chord.Chord C5>\n",
            "{16.0} <music21.chord.Chord B-4>\n",
            "{16.5} <music21.chord.Chord B-4>\n",
            "{16.75} <music21.chord.Chord A4>\n",
            "{17.0} <music21.chord.Chord B-4>\n",
            "{17.5} <music21.chord.Chord B-4>\n",
            "{17.75} <music21.chord.Chord B-4>\n",
            "{18.0} <music21.chord.Chord B-4>\n",
            "{18.5} <music21.chord.Chord B-4>\n",
            "{18.75} <music21.chord.Chord B-4>\n",
            "{19.0} <music21.chord.Chord B-4>\n",
            "{19.5} <music21.chord.Chord F4>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating music with temperature 0.42\n",
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens=50, temperature=0.42\n",
        ")\n",
        "midi_stream_three = info[-1][\"midi\"].chordify()\n",
        "midi_stream_three.show('text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rv8GHIVF7ad",
        "outputId": "035c8cfc-8b3a-4da6-95ae-67a9c07a0fa8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.0} <music21.clef.BassClef>\n",
            "{0.0} <music21.key.Key of C major>\n",
            "{0.0} <music21.meter.TimeSignature 1/8>\n",
            "{0.0} <music21.chord.Chord A4>\n",
            "{0.5} <music21.meter.TimeSignature 3/2>\n",
            "{0.5} <music21.chord.Chord A4>\n",
            "{0.75} <music21.chord.Chord A4>\n",
            "{1.0} <music21.chord.Chord A4>\n",
            "{1.25} <music21.chord.Chord A4>\n",
            "{1.5} <music21.chord.Chord A4>\n",
            "{1.75} <music21.chord.Chord A4>\n",
            "{2.0} <music21.chord.Chord E4>\n",
            "{2.25} <music21.chord.Chord C#3>\n",
            "{2.5} <music21.chord.Chord D3>\n",
            "{2.75} <music21.chord.Chord E4>\n",
            "{3.0} <music21.note.Rest 16th>\n",
            "{3.25} <music21.chord.Chord F#4>\n",
            "{3.5} <music21.chord.Chord F#4>\n",
            "{3.75} <music21.chord.Chord F#4>\n",
            "{4.0} <music21.chord.Chord F#4>\n",
            "{4.25} <music21.chord.Chord G4>\n",
            "{4.5} <music21.chord.Chord F#4>\n",
            "{4.75} <music21.chord.Chord D4>\n",
            "{5.0} <music21.chord.Chord E4>\n",
            "{5.25} <music21.chord.Chord F4>\n",
            "{5.5} <music21.chord.Chord G4>\n",
            "{5.75} <music21.chord.Chord B-4>\n",
            "{6.0} <music21.chord.Chord A4>\n",
            "{6.25} <music21.chord.Chord G4>\n",
            "{6.5} <music21.chord.Chord C5>\n",
            "{6.75} <music21.chord.Chord C5>\n",
            "{7.0} <music21.chord.Chord F#4>\n",
            "{7.25} <music21.chord.Chord G4>\n",
            "{7.5} <music21.chord.Chord D4>\n",
            "{7.75} <music21.chord.Chord F4>\n",
            "{8.0} <music21.chord.Chord G4>\n",
            "{8.25} <music21.chord.Chord A4>\n",
            "{8.5} <music21.chord.Chord B-4>\n",
            "{8.75} <music21.chord.Chord C5>\n",
            "{9.0} <music21.chord.Chord D5>\n",
            "{9.25} <music21.chord.Chord B-4>\n",
            "{9.5} <music21.chord.Chord A4>\n",
            "{9.75} <music21.chord.Chord G4>\n",
            "{10.0} <music21.chord.Chord A4>\n",
            "{10.25} <music21.chord.Chord B-4>\n",
            "{10.5} <music21.chord.Chord E5>\n",
            "{10.75} <music21.chord.Chord B-4>\n",
            "{11.0} <music21.chord.Chord A4>\n",
            "{11.25} <music21.chord.Chord G4>\n",
            "{11.5} <music21.chord.Chord F4>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating music with temperature 1 and 100 tokens for fun\n",
        "info = music_generator.generate(\n",
        "    [\"START\"], [\"0.0\"], max_tokens=100, temperature=1\n",
        ")\n",
        "midi_stream_four = info[-1][\"midi\"].chordify()\n",
        "midi_stream_four.show('text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2yGSc30HTcD",
        "outputId": "71f60abe-a14d-4d9e-a9b8-312bc374a19e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0.0} <music21.clef.BassClef>\n",
            "{0.0} <music21.key.Key of C major>\n",
            "{0.0} <music21.meter.TimeSignature 1/4>\n",
            "{0.0} <music21.chord.Chord E4>\n",
            "{0.3333} <music21.note.Rest 2/3ql>\n",
            "{1.0} <music21.meter.TimeSignature 4/4>\n",
            "{1.0} <music21.chord.Chord G5>\n",
            "{1.5} <music21.chord.Chord G5>\n",
            "{2.0} <music21.chord.Chord B2>\n",
            "{2.25} <music21.chord.Chord C3>\n",
            "{2.5} <music21.chord.Chord G4>\n",
            "{4.0} <music21.chord.Chord A4>\n",
            "{4.1667} <music21.chord.Chord A4>\n",
            "{4.25} <music21.chord.Chord B2>\n",
            "{4.5} <music21.chord.Chord G5>\n",
            "{4.75} <music21.chord.Chord B-4>\n",
            "{5.0} <music21.chord.Chord A5>\n",
            "{5.25} <music21.chord.Chord F#5>\n",
            "{5.75} <music21.chord.Chord G5>\n",
            "{6.0} <music21.chord.Chord G5>\n",
            "{6.25} <music21.chord.Chord G5>\n",
            "{7.5} <music21.chord.Chord G5>\n",
            "{7.75} <music21.chord.Chord G5>\n",
            "{7.9167} <music21.chord.Chord C5>\n",
            "{8.0} <music21.chord.Chord D5>\n",
            "{8.5} <music21.chord.Chord F#5>\n",
            "{9.25} <music21.chord.Chord G5>\n",
            "{9.5} <music21.chord.Chord G5>\n",
            "{10.0} <music21.chord.Chord G5>\n",
            "{10.1667} <music21.chord.Chord D5>\n",
            "{10.5} <music21.chord.Chord C5>\n",
            "{10.75} <music21.chord.Chord D5>\n",
            "{11.0} <music21.chord.Chord E5>\n",
            "{11.5} <music21.chord.Chord F5>\n",
            "{12.0} <music21.chord.Chord G5>\n",
            "{12.0833} <music21.chord.Chord A5>\n",
            "{12.25} <music21.chord.Chord F5>\n",
            "{12.75} <music21.chord.Chord B-5>\n",
            "{13.0} <music21.chord.Chord A5>\n",
            "{13.25} <music21.chord.Chord G5>\n",
            "{13.75} <music21.chord.Chord F5>\n",
            "{14.25} <music21.chord.Chord E5>\n",
            "{14.75} <music21.chord.Chord D5>\n",
            "{14.9167} <music21.chord.Chord C5>\n",
            "{15.0} <music21.chord.Chord F5>\n",
            "{15.25} <music21.chord.Chord F5>\n",
            "{15.5833} <music21.chord.Chord F5>\n",
            "{15.75} <music21.chord.Chord F5>\n",
            "{16.0} <music21.chord.Chord F5>\n",
            "{16.25} <music21.chord.Chord F5>\n",
            "{16.5} <music21.chord.Chord F5>\n",
            "{17.0} <music21.chord.Chord F5>\n",
            "{17.5} <music21.chord.Chord F5>\n",
            "{18.0} <music21.chord.Chord B4>\n",
            "{18.75} <music21.chord.Chord E5>\n",
            "{19.0} <music21.chord.Chord F5>\n",
            "{19.25} <music21.chord.Chord G5>\n",
            "{19.5} <music21.chord.Chord D5>\n",
            "{19.75} <music21.chord.Chord E-5>\n",
            "{20.0} <music21.chord.Chord E5>\n",
            "{20.25} <music21.chord.Chord C5>\n",
            "{20.5} <music21.chord.Chord F5>\n",
            "{21.0} <music21.chord.Chord E5>\n",
            "{21.5} <music21.chord.Chord G5>\n",
            "{22.0} <music21.chord.Chord F5>\n",
            "{22.25} <music21.chord.Chord D5>\n",
            "{22.5} <music21.chord.Chord A5>\n",
            "{22.75} <music21.chord.Chord F5>\n",
            "{23.0} <music21.chord.Chord A5>\n",
            "{28.3333} <music21.chord.Chord E5>\n",
            "{28.8333} <music21.chord.Chord C5>\n",
            "{29.0} <music21.chord.Chord G#5>\n",
            "{29.25} <music21.chord.Chord F#5>\n",
            "{29.5} <music21.chord.Chord E5>\n",
            "{29.75} <music21.chord.Chord A5>\n",
            "{30.0} <music21.chord.Chord A5>\n",
            "{30.5} <music21.chord.Chord G5>\n",
            "{31.0} <music21.chord.Chord D5>\n",
            "{31.25} <music21.chord.Chord E-5>\n",
            "{31.5} <music21.chord.Chord A3>\n",
            "{32.25} <music21.chord.Chord G5>\n",
            "{32.5} <music21.chord.Chord C#5>\n",
            "{32.75} <music21.chord.Chord A3>\n",
            "{33.25} <music21.chord.Chord A4>\n",
            "{33.4167} <music21.chord.Chord D5>\n",
            "{33.5833} <music21.chord.Chord F5>\n",
            "{33.75} <music21.chord.Chord F5>\n",
            "{33.9167} <music21.chord.Chord G#5>\n",
            "{34.0833} <music21.chord.Chord F5>\n",
            "{34.5833} <music21.chord.Chord A4>\n",
            "{34.8333} <music21.chord.Chord D5>\n",
            "{35.3333} <music21.chord.Chord B-4>\n",
            "{36.5833} <music21.chord.Chord C#5>\n",
            "{36.8333} <music21.chord.Chord E5>\n",
            "{37.0833} <music21.chord.Chord F5>\n",
            "{37.3333} <music21.chord.Chord D5>\n",
            "{38.0833} <music21.chord.Chord F5>\n",
            "{38.3333} <music21.chord.Chord E5>\n",
            "{38.5833} <music21.chord.Chord A3>\n",
            "{38.8333} <music21.chord.Chord B4>\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1uGdDmzhkH0ScgGYsqMS9Zt_wqcvFNlt1",
      "authorship_tag": "ABX9TyOlTp5aiGx2jHw5ssgByJhR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}